{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/sk-classroom/asc-bert/blob/main/assignments/assignment_01.ipynb)\n",
    "\n",
    "We will learn how to generate word embeddings using BERT. BERT produces contextualized word embeddings, where the embeddings are computed based on the context of the word. Thus, a single word can have different embeddings based on its context. \n",
    "\n",
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you haven't installed the required packages, please install them using pip\n",
    "#!pip install transformers plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data \n",
    "\n",
    "We will use [CoarseWSD-20](https://github.com/danlou/bert-disambiguation/tree/master/data/CoarseWSD-20). The dataset contains sentences with polysemous words and their sense labels. We will see how to use BERT to disambiguate the word senses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_pos</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>25</td>\n",
       "      <td>transparency reports are issued today by a var...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>72</td>\n",
       "      <td>maximum files per server : 16 million maximum ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>0</td>\n",
       "      <td>apple , google , and intel are among 1,600 tec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>17</td>\n",
       "      <td>the town was first settled around 1763 by jean...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>18</td>\n",
       "      <td>it is made of puff pastry and four fillings : ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2233</th>\n",
       "      <td>20</td>\n",
       "      <td>past projects the firm has contributed to the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>43</td>\n",
       "      <td>nutrients and potential health effects sea-buc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>13</td>\n",
       "      <td>wheat is the most commonly grown product ; how...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>16</td>\n",
       "      <td>launched in 1981 by london-based rainbow softw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656</th>\n",
       "      <td>17</td>\n",
       "      <td>technical background with the transition to ta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_pos                                           sentence  label\n",
       "988         25  transparency reports are issued today by a var...      0\n",
       "879         72  maximum files per server : 16 million maximum ...      0\n",
       "1101         0  apple , google , and intel are among 1,600 tec...      0\n",
       "1958        17  the town was first settled around 1763 by jean...      1\n",
       "587         18  it is made of puff pastry and four fillings : ...      1\n",
       "2233        20  past projects the firm has contributed to the ...      0\n",
       "794         43  nutrients and potential health effects sea-buc...      1\n",
       "1423        13  wheat is the most commonly grown product ; how...      1\n",
       "2122        16  launched in 1981 by london-based rainbow softw...      0\n",
       "1656        17  technical background with the transition to ta...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(focal_word, is_train, n_samples=100):\n",
    "    data_type = \"train\" if is_train else \"test\"\n",
    "    data_file = f\"https://raw.githubusercontent.com/danlou/bert-disambiguation/master/data/CoarseWSD-20/{focal_word}/{data_type}.data.txt\"\n",
    "    label_file = f\"https://raw.githubusercontent.com/danlou/bert-disambiguation/master/data/CoarseWSD-20/{focal_word}/{data_type}.gold.txt\"\n",
    "\n",
    "    data_table = pd.read_csv(\n",
    "        data_file,\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "        dtype={\"word_pos\": int, \"sentence\": str},\n",
    "        names=[\"word_pos\", \"sentence\"],\n",
    "    )\n",
    "    label_table = pd.read_csv(\n",
    "        label_file,\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "        dtype={\"label\": int},\n",
    "        names=[\"label\"],\n",
    "    )\n",
    "    combined_table = pd.concat([data_table, label_table], axis=1)\n",
    "    return combined_table.sample(n_samples)\n",
    "\n",
    "\n",
    "focal_word = \"apple\"\n",
    "\n",
    "train_data = load_data(focal_word, is_train=True)\n",
    "\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to the [README](https://github.com/danlou/bert-disambiguation/blob/master/data/CoarseWSD-20/README.txt) for the data. \n",
    "\n",
    "## Define BERT model\n",
    "\n",
    "We will use `transformers` library developed by Hugging Face to define the BERT model. To use the model, we will need:  \n",
    "1. BERT tokenizer that converts the text into tokens. \n",
    "2. BERT model that computes the embeddings of the tokens. \n",
    "\n",
    "We will use the `bert-base-uncased` model and tokenizer. Let's define the model and tokenizer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Define the model and tokenizer \n",
    "# Hint: Use the transformers library to load a pre-trained BERT model and tokenizer\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With BERT, we need to prepare text in ways that BERT can understand. \n",
    "Specifically, we prepend it with ```[CLS]``` and append ```[SEP]```. We will then convert the text to a tensor of token ids, which is ready to be fed into the model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(text):\n",
    "    text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    segments_ids = torch.ones((1, len(indexed_tokens)), dtype=torch.long)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = segments_ids.clone()\n",
    "    return tokenized_text, tokens_tensor, segments_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What is segment tensor?\n",
    "BERT models are designed to process sentence pairs, differentiated by 0s and 1s to indicate the first and second sentence respectively. In the case of single-sentence inputs, we assign a vector of 1s to each token, indicating they all belong to the first sentence.\n",
    "\n",
    "Let's get the BERT embeddings for the sentence \"Bank is located in the city of London\". \n",
    "\n",
    "First, let's prepare the text for BERT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'bank', 'is', 'located', 'in', 'the', 'city', 'of', 'london', '[SEP]']\n",
      "tensor([[ 101, 2924, 2003, 2284, 1999, 1996, 2103, 1997, 2414,  102]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "text = \"Bank is located in the city of London\"\n",
    "tokenized_text, tokens_tensor, segments_tensor = prepare_text(text)\n",
    "print(tokenized_text)\n",
    "print(tokens_tensor)\n",
    "print(segments_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's get the BERT embeddings for each token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(tokens_tensor, segments_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output includes `loss`, `logits`, and `hidden_states`. We will use `hidden_states`, which contains the embeddings of the tokens. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many layers?  13\n",
      "Shape?  torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "hidden_states = outputs.hidden_states\n",
    "\n",
    "print(\"how many layers? \", len(hidden_states))\n",
    "print(\"Shape? \", hidden_states[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden states are a list of 13 tensors, where each tensor is of shape (batch_size, sequence_length, hidden_size). The first tensor is the input embeddings, and the subsequent tensors are the hidden states of the BERT layers. \n",
    "\n",
    "So, we have 13 choice of hidden states. Deep layers close to the output capture the context of the word from the previous layers.\n",
    "\n",
    "Here we will take the average over the last four hidden states for each token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Compute the embedding of the token\n",
    "emb = hidden_states[0].squeeze(0)[0]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emb is of shape (sequence_length, hidden_size). Let us summarize the embeddings of the tokens into a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(text):\n",
    "    tokenized_text, tokens_tensor, segments_tensor = prepare_text(text)\n",
    "    outputs = model(tokens_tensor, segments_tensor)\n",
    "    hidden_states = outputs.hidden_states\n",
    "    emb = hidden_states[0].squeeze(0)[0]\n",
    "    return emb, tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "Let's embed the text and get the embedding of the focal word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 20.54it/s]\n"
     ]
    }
   ],
   "source": [
    "labels = []  # label\n",
    "emb = []  # embedding\n",
    "sentences = []  # sentence\n",
    "\n",
    "# TODO: Go through the data and get the embedding of the focal word.\n",
    "for i, row in tqdm(train_data.iterrows(), total=len(train_data)):\n",
    "    text = row[\"sentence\"]\n",
    "    word_pos = row[\"word_pos\"]\n",
    "    label = row[\"label\"]\n",
    "    tokenized_text, tokens_tensor, segments_tensor = prepare_text(text)\n",
    "    \n",
    "    # Pass tokens and segments through the model\n",
    "    with torch.no_grad():  # Ensure no gradient tracking\n",
    "        outputs = model(tokens_tensor, segments_tensor)\n",
    "        hidden_states = outputs.hidden_states\n",
    "    \n",
    "    # Extract the embedding of the focal word\n",
    "    word_embedding = hidden_states[0].squeeze(0)[word_pos].detach().numpy()\n",
    "    \n",
    "    # Append data to lists\n",
    "    emb.append(word_embedding)\n",
    "    labels.append(label)\n",
    "    sentences.append(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results \n",
    "\n",
    "Let's plot the embeddings of the focal word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(emb, labels, sentences):\n",
    "    \n",
    "    xy = PCA(n_components=2).fit_transform(emb)\n",
    "    \n",
    "    fig = px.scatter(\n",
    "        x=xy[:, 0],\n",
    "        y=xy[:, 1],\n",
    "        color=labels,\n",
    "        hover_data=[sentences],\n",
    "        title=\"PCA of Word Embeddings\",\n",
    "    )\n",
    "    fig.update_layout(width=700, height=500)\n",
    "    fig.update_traces(\n",
    "        marker=dict(size=12, line=dict(width=2, color=\"DarkSlateGrey\")),\n",
    "        selector=dict(mode=\"markers\"),\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "plot_result(emb, labels, sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "def save_assignment(emb, labels, assignment_id, data_dir):\n",
    "    K = len(set(labels))\n",
    "    xy = LinearDiscriminantAnalysis(n_components=K - 1).fit_transform(emb, labels)\n",
    "    xy_df = pd.DataFrame(xy)\n",
    "    xy_df[\"label\"] = labels\n",
    "    xy_df.to_csv(f\"{data_dir}/eval_test_{assignment_id}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "- Run this notebook for any word available in [the dataset](https://github.com/danlou/bert-disambiguation/tree/master/data/CoarseWSD-20) except for \"apple\". \n",
    "- Save the (dimensionality-reduced) embeddings of the test data and their labels by running the following cell.  \n",
    "- Make sure to place the generated file \"eval_test_01.csv\" in the \"data\" folder. \n",
    "- Commit the file to your assignment repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "focal_word = \"crane\"\n",
    "test_data = load_data(focal_word, is_train=False)\n",
    "\n",
    "emb = []\n",
    "labels = []\n",
    "sentences = []\n",
    "\n",
    "for i, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "    text = row[\"sentence\"]\n",
    "    word_pos = row[\"word_pos\"]\n",
    "    label = row[\"label\"]\n",
    "    tokenized_text, tokens_tensor, segments_tensor = prepare_text(text)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensor)\n",
    "        hidden_states = outputs.hidden_states\n",
    "    \n",
    "    word_embedding = hidden_states[0].squeeze(0)[word_pos].detach().numpy()\n",
    "    \n",
    "    emb.append(word_embedding)\n",
    "    labels.append(label)\n",
    "    sentences.append(tokenized_text)\n",
    "\n",
    "plot_result(emb, labels, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_assignment(emb, labels, assignment_id=\"01\", data_dir=\"../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "\n",
    "- Use the same dataset as Assignment 1. \n",
    "- Compute the word embedding using the first hidden state of the BERT model \n",
    "- Save the embeddings of the test data and their labels by running the following cell.  \n",
    "- Make sure to place the generated file \"eval_test_02.csv\" in the \"data\" folder. \n",
    "- Commit the file to your assignment repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code:\n",
    "# - Use the same dataset as Assignment 1. \n",
    "# Compute the word embedding using the first hidden state of the BERT model \n",
    "\n",
    "focal_word = \"crane\"\n",
    "train_data = load_data(focal_word, is_train=True)\n",
    "\n",
    "labels = []  # label\n",
    "emb = []  # embedding\n",
    "\n",
    "for i, row in tqdm(train_data.iterrows(), total=len(train_data)):\n",
    "    text = row[\"sentence\"]\n",
    "    word_pos = row[\"word_pos\"]\n",
    "    label = row[\"label\"]\n",
    "    tokenized_text, tokens_tensor, segments_tensor = prepare_text(text)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensor)\n",
    "        hidden_states = outputs.hidden_states\n",
    "    \n",
    "    word_embedding = hidden_states[1].squeeze(0)[word_pos].detach().numpy()\n",
    "    \n",
    "    emb.append(word_embedding)\n",
    "    labels.append(label)\n",
    "    \n",
    "plot_result(emb, labels, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_assignment(emb, labels, assignment_id=\"02\", data_dir=\"../data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applsoftcomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
